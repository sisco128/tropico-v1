tropico-v1
├── Dockerfile
├── app.py
├── db.py
├── requirements.txt
├── subdomain_discovery.py
├── tasks.py
└── worker.py

===== app.py =====
# app.py
from flask import Flask, request, jsonify
from redis import Redis
from rq import Queue
import os

from db import init_db, create_scan, get_scan
from tasks import discover_subdomains

app = Flask(__name__)

# Initialize DB (creates tables if needed)
init_db()

# Configure Redis
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
redis_conn = Redis.from_url(REDIS_URL)
q = Queue("default", connection=redis_conn)

@app.route("/scans", methods=["POST"])
def create_scan_api():
    data = request.get_json()
    domain = data.get("domain")
    if not domain:
        return jsonify({"error": "domain is required"}), 400

    # 1) Insert a new 'scan' row
    scan_id = create_scan(domain)
    # 2) Enqueue the new combined job
    job = q.enqueue(discover_subdomains, scan_id, domain)
    return jsonify({"scan_id": scan_id, "job_id": job.get_id()}), 201

@app.route("/scans/<int:scan_id>", methods=["GET"])
def get_scan_api(scan_id):
    scan_data = get_scan(scan_id)
    if not scan_data:
        return jsonify({"error": "Not found"}), 404
    return jsonify(scan_data)

if __name__ == "__main__":
    # For local dev run:
    # python app.py
    app.run(host="0.0.0.0", port=8000, debug=True)



===== worker.py =====
# worker.py
import os
from rq import Worker, Queue, Connection
from redis import Redis

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
listen = ["default"]

redis_conn = Redis.from_url(REDIS_URL)

if __name__ == "__main__":
    with Connection(redis_conn):
        worker = Worker(map(Queue, listen))
        worker.work()



===== db.py =====
# db.py
import os
import psycopg2

def get_connection():
    """
    Connect to Postgres using DATABASE_URL or fallback DSN with 'localhost'.
    """
    db_url = os.getenv("DATABASE_URL") or "postgres://siscolo:@localhost:5432/my_local_db"
    return psycopg2.connect(db_url)

def init_db():
    """
    Create the tables if they don't exist (scans, subdomains, endpoints).
    """
    conn = get_connection()
    cur = conn.cursor()

    # scans table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS scans (
            id SERIAL PRIMARY KEY,
            domain VARCHAR(255) NOT NULL,
            status VARCHAR(20) NOT NULL DEFAULT 'queued',
            created_at TIMESTAMP NOT NULL DEFAULT NOW()
        );
    """)

    # subdomains table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS subdomains (
            id SERIAL PRIMARY KEY,
            scan_id INTEGER NOT NULL REFERENCES scans(id),
            subdomain VARCHAR(255) NOT NULL,
            created_at TIMESTAMP NOT NULL DEFAULT NOW()
        );
    """)

    # endpoints table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS endpoints (
            id SERIAL PRIMARY KEY,
            scan_id INTEGER NOT NULL REFERENCES scans(id),
            subdomain VARCHAR(255) NOT NULL,
            url TEXT NOT NULL,
            status_code INTEGER,
            content_type VARCHAR(255),
            server VARCHAR(255),
            framework VARCHAR(255),
            created_at TIMESTAMP NOT NULL DEFAULT NOW()
        );
    """)

    conn.commit()
    cur.close()
    conn.close()

def create_scan(domain):
    """
    Insert a new row in the 'scans' table and return its ID.
    """
    conn = get_connection()
    cur = conn.cursor()
    cur.execute("INSERT INTO scans (domain) VALUES (%s) RETURNING id;", (domain,))
    scan_id = cur.fetchone()[0]
    conn.commit()
    cur.close()
    conn.close()
    return scan_id

def get_scan(scan_id):
    """
    Return the scan record plus subdomains (list) and endpoints (flattened list).
    {
      "id": ...,
      "domain": ...,
      "status": ...,
      "created_at": ...,
      "subdomains": [...],
      "endpoints": [
        {
          "subdomain": "...",
          "url": "...",
          "status_code": ...,
          "content_type": ...,
          "server": ...,
          "framework": ...
        },
        ...
      ]
    }
    """
    conn = get_connection()
    cur = conn.cursor()

    # fetch the scan row
    cur.execute("""
        SELECT id, domain, status, created_at
        FROM scans
        WHERE id = %s
    """, (scan_id,))
    row = cur.fetchone()
    if not row:
        cur.close()
        conn.close()
        return None

    scan_data = {
        "id": row[0],
        "domain": row[1],
        "status": row[2],
        "created_at": str(row[3])
    }

    # fetch subdomains
    cur.execute("SELECT subdomain FROM subdomains WHERE scan_id = %s", (scan_id,))
    subdomain_rows = cur.fetchall()
    scan_data["subdomains"] = [r[0] for r in subdomain_rows]

    # fetch endpoints as a flattened list
    cur.execute("""
        SELECT subdomain, url, status_code, content_type, server, framework
        FROM endpoints
        WHERE scan_id = %s
    """, (scan_id,))
    endpoint_rows = cur.fetchall()

    endpoints_list = []
    for (subd, url, st_code, ctype, srv, fw) in endpoint_rows:
        endpoints_list.append({
            "subdomain": subd,
            "url": url,
            "status_code": st_code,
            "content_type": ctype,
            "server": srv,
            "framework": fw
        })

    scan_data["endpoints"] = endpoints_list

    cur.close()
    conn.close()
    return scan_data

def update_scan_status(scan_id, status):
    """
    Update the 'status' field in the 'scans' table.
    """
    conn = get_connection()
    cur = conn.cursor()
    cur.execute("UPDATE scans SET status=%s WHERE id=%s;", (status, scan_id))
    conn.commit()
    cur.close()
    conn.close()

def insert_subdomain(scan_id, subdomain):
    """
    Insert a discovered subdomain into 'subdomains' table.
    """
    conn = get_connection()
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO subdomains (scan_id, subdomain)
        VALUES (%s, %s)
        ON CONFLICT DO NOTHING
    """, (scan_id, subdomain))
    conn.commit()
    cur.close()
    conn.close()

def insert_endpoint(scan_id, subdomain, endpoint_data):
    """
    Insert a discovered endpoint (URL, status_code, content_type, etc.) into 'endpoints' table.
    """
    conn = get_connection()
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO endpoints (scan_id, subdomain, url, status_code, content_type, server, framework)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT DO NOTHING
    """, (
        scan_id,
        subdomain,
        endpoint_data["url"],
        endpoint_data["status_code"],
        endpoint_data["content_type"],
        endpoint_data["server"],
        endpoint_data["framework"],
    ))
    conn.commit()
    cur.close()
    conn.close()



===== Dockerfile =====
# Dockerfile
FROM python:3.11-slim

# 1) Use the official Playwright + Python base
FROM mcr.microsoft.com/playwright/python:v1.35.0-focal

# 2) Install packages needed for subfinder (wget, unzip)
RUN apt-get update && apt-get install -y wget unzip && rm -rf /var/lib/apt/lists/*

# 3) Download & install subfinder (pick a valid version)
RUN wget https://github.com/projectdiscovery/subfinder/releases/download/v2.6.7/subfinder_2.6.7_linux_amd64.zip -O subfinder.zip \
    && unzip subfinder.zip \
    && mv subfinder /usr/local/bin/subfinder \
    && chmod +x /usr/local/bin/subfinder \
    && rm subfinder.zip

WORKDIR /app

# 3) Copy requirements & install Python deps
COPY requirements.txt /app/
RUN pip install --no-cache-dir -r requirements.txt

# 4) Install Playwright’s browsers (Chromium)
RUN playwright install --with-deps chromium

# 5) Copy the rest of your app code
COPY . /app

# Expose port 8000 for Flask/gunicorn
EXPOSE 8000

# Default command: run gunicorn on port 8000
CMD ["gunicorn", "-b", "0.0.0.0:8000", "app:app"]



===== tasks.py =====
# tasks.py
from db import insert_subdomain, update_scan_status, insert_endpoint
from subdomain_discovery import run_subfinder

from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin

def discover_subdomains(scan_id, domain):
    """
    1) Subfinder for subdomains
    2) For each subdomain -> discover endpoints -> analyze -> store in DB
    3) Mark 'complete' or 'error'
    """
    try:
        # 1) Subdomain discovery
        subdomains = run_subfinder(domain)
        for sd in subdomains:
            insert_subdomain(scan_id, sd)

        # 2) For each subdomain, discover & analyze endpoints
        for subd in subdomains:
            discovered_urls = discover_endpoints(subd)
            for url in discovered_urls:
                ep_data = analyze_api(url)
                if ep_data:
                    insert_endpoint(scan_id, subd, ep_data)

        # 3) Mark completed
        update_scan_status(scan_id, 'complete')

    except Exception as e:
        print(f"Error in discover_subdomains: {e}")
        update_scan_status(scan_id, 'error')


def discover_endpoints(subdomain):
    """
    Use Playwright + BeautifulSoup to gather all 'href' and 'src' from <a> and <script>.
    Returns a list of absolute URLs.
    """
    discovered_urls = []
    url = f"https://{subdomain}"
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            try:
                page.goto(url, timeout=5000)
            except Exception as e:
                print(f"Error loading page {url}: {e}")
                browser.close()
                return []

            soup = BeautifulSoup(page.content(), "html.parser")
            links = [a.get("href") for a in soup.find_all("a", href=True)]
            scripts = [s.get("src") for s in soup.find_all("script", src=True)]

            all_urls = []
            for link in links + scripts:
                if link:
                    full_url = urljoin(url, link)
                    all_urls.append(full_url)

            discovered_urls = list(set(all_urls))
            browser.close()
    except Exception as e:
        print(f"Error discovering endpoints on {subdomain}: {e}")

    return discovered_urls

def analyze_api(url):
    """
    Makes a GET request to the URL, returns basic info as dict or None if failed.
    """
    try:
        r = requests.get(url, timeout=5)
        return {
            "url": url,
            "status_code": r.status_code,
            "content_type": r.headers.get("Content-Type", "Unknown"),
            "server": r.headers.get("Server", "Unknown"),
            "framework": "Unknown",
        }
    except Exception as e:
        print(f"Error analyzing URL {url}: {e}")
        return None



===== subdomain_discovery.py =====
# subdomain_discovery.py
import subprocess
import json

def run_subfinder(domain):
    """
    Runs subfinder and returns a list of discovered subdomains.
    Subfinder must be in PATH (/usr/local/bin/subfinder).
    """
    try:
        cmd = ["subfinder", "-d", domain, "-oJ"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"Error running subfinder: {result.stderr}")
            return []

        subdomains = []
        for line in result.stdout.splitlines():
            if line.strip():
                data = json.loads(line)
                host = data.get("host")
                if host:
                    subdomains.append(host)
        return subdomains

    except Exception as e:
        print(f"Exception in run_subfinder: {e}")
        return []



===== requirements.txt =====
Flask==2.3.2
redis==4.6.0
rq==1.13.0
psycopg2-binary==2.9.6
sqlalchemy==2.0.21
gunicorn==21.2.0

requests==2.31.0
beautifulsoup4==4.12.2
playwright==1.35.0



